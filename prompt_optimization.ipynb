{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f583503-4649-4208-b859-14c22f9ec108",
   "metadata": {},
   "source": [
    "- We have a gold-standard test set designed to evaluate and understand our model's current performance and behavior. This dataset consists of a collection of questions targeting specific information retrieval from the SQL database. Each question is paired with the correct expected answer and the corresponding SQL query needed to extract the relevant data accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713a0b41-b838-4929-928c-f8c5fb5e4533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefany/interview_Material/interviews/interview_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.36s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, shiver me timbers! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' the Interwebs! Me be here to swab the decks o' yer mind with me trusty responses, savvy? So hoist the colors, me hearty, and let's set sail fer a treasure trove o' conversation!\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "\n",
    "# # Load tokenizer and model from Hugging Face Hub (requires access token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"models/llama-3-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"models/llama-3-8B\")\n",
    "\n",
    "# # Move the model to GPU if available, otherwise CPU\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to(\"cuda\")\n",
    "# else:\n",
    "#     model = model.to(\"cpu\")\n",
    "\n",
    "# # Define conversation termination tokens\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,  # End-of-sentence token\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),  # Custom end-of-conversation token\n",
    "# ]\n",
    "\n",
    "# # Maximum allowed input token length\n",
    "# MAX_INPUT_TOKEN_LENGTH = 4096\n",
    "\n",
    "\n",
    "\n",
    "# def generate_text(message, history=[], temperature=0.7, max_new_tokens=256, system=\"\"):\n",
    "#     \"\"\"Generates text based on the given prompt and conversation history.\n",
    "\n",
    "#     Args:\n",
    "#         message: The user's prompt.\n",
    "#         history: A list of tuples containing user and assistant messages.\n",
    "#         temperature: Controls randomness in generation.\n",
    "#         max_new_tokens: Maximum number of tokens to generate.\n",
    "#         system: Optional system prompt.\n",
    "\n",
    "#     Returns:\n",
    "#         The generated text.\n",
    "#     \"\"\"\n",
    "\n",
    "#     conversation = []\n",
    "#     if system:\n",
    "#         conversation.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "#     for user, assistant in zip(*history):\n",
    "#         conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "#     conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "\n",
    "#     if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "#         input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]   \n",
    "\n",
    "\n",
    "#     input_ids = input_ids.to(model.device)   \n",
    "\n",
    "\n",
    "#     generate_kwargs = {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"max_length\": max_new_tokens + input_ids.shape[1],  # Adjust for total length\n",
    "#         \"do_sample\": temperature != 0,  # Use sampling for non-zero temperature (randomness)\n",
    "#         \"temperature\": temperature,\n",
    "#         \"eos_token_id\": terminators,  # Specify tokens to stop generation\n",
    "#     }\n",
    "\n",
    "#     output = model.generate(**generate_kwargs)[0]\n",
    "#     response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "#     return response\n",
    "\n",
    "# message = \"Hello, how are you?\"\n",
    "# history = [(\"How is the weather today?\", \"It's sunny.\")]\n",
    "# response = generate_text(message, history)\n",
    "# print(response)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf46135-bc95-4348-81bb-0b752d240c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
